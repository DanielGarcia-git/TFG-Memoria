@book{alma991003132729706711,
    title     = {Reversing : secrets of reverse engineering },
    year      = {2005},
    keywords  = {Enginyeria del software},
    author    = {Eilam, Eldad and Chikofsky, Elliot J.},
    address   = {Indianapolis},
    booktitle = {Reversing : secrets of reverse engineering},
    isbn      = {0764574817},
    language  = {eng},
    publisher = {Wiley}
}

@book{alma991004951313206711,
    title     = {Reverse engineering },
    year      = {1996},
    keywords  = {Software engineering},
    abstract  = {Reverse Engineering brings together in one place important contributions and up-to-date research results in this important area. Reverse Engineering serves as an excellent reference, providing insight into some of the most important issues in the field.},
    author    = {Newcomb, Philip and Wills, Linda},
    address   = {Boston},
    booktitle = {Reverse engineering},
    edition   = {1st ed. 1996.},
    isbn      = {0-585-27477-0},
    language  = {eng},
    publisher = {Kluwer Academic Publishers}
}

@book{alma991004951382906711,
    series    = {Monographs in Computer Science},
    title     = {Reverse Engineering of Object Oriented Code},
    year      = {2005},
    keywords  = {Software engineering},
    abstract  = {In software evolution and maintenance, the ultimate, most reliable description of a system is its source code. Reverse engineering aims at extracting abstract, goal-oriented views from the code, to summarize relevant properties of program computations. Reverse Engineering of Object Oriented Code presents a unifying framework for the analysis of object oriented code. Using Unified Modeling Language (UML) to represent the extracted design diagrams, the book explains how to recover them from object oriented code, thereby enabling developers to better comprehend their product and evaluate the impact of changes to it. Furthermore, it describes the algorithms involved in recovering views and demonstrates some of the techniques that can be employed for their visualization. The presentation is fully self-contained. Topics and Features: *Provides unique, in-depth exposition of the core concepts, principles, and methods behind reverse engineering object oriented code *Explains the techniques and algorithms through numerous examples of object oriented code, the leading programming paradigm *Focuses on fully automated design recovery, and deals with static and dynamic source-code analysis algorithms *Explores code-centered analysis to obtain design diagrams aligned with the implementation *Describes structural and behavioral views to offer a multi-perspective assessment of the system being analyzed *Reports the analysis results in UML, the standard language for representing design diagrams in object oriented program development This new state-of-the-art volume covers core methodologies for reverse engineering object oriented code, allowing for improved control in future code maintenance and modification. It is a significant resource for researchers and software engineers in the areas of reverse engineering, code analysis, object oriented programming, and UML. In addition, it will be invaluable as the reference book for advanced courses in these areas.},
    author    = {Tonella, Paolo. and Potrich, Alessandra.},
    address   = {New York, NY},
    edition   = {1st ed. 2005.},
    isbn      = {1-280-19032-9},
    language  = {eng},
    publisher = {Springer New York}
}

@article{Artículo_doctrinal,
    abstract  = {La inteligencia artificial está suscitando intensos debates en
                 el conjunto de la doctrina sobre su compatibilidad con la ética humanista
                 imperante en la sociedad occidental. Esta perspectiva antropocéntrica del
                 cambio de paradigma planteado por la inteligencia artificial no agota sin
                 embargo la hilera de consecuencias perniciosas que la misma puede acompañar.
                 Así pues, el medio ambiente también se erige como un bien jurídico a proteger
                 frente al fenómeno de esta nueva tecnología. La ingente cantidad de energía demandada
                 por algunos de los modelos de inteligencia artificial erosiona de raíz cualquier objetivo
                 de eficiencia energética que pueda llegar a plantearse en el campo de las TIC. Por ello,
                 el objetivo de este trabajo será el consistente en analizar este nuevo reto al que deberá
                 enfrentarse, más pronto que tarde, el Derecho ambiental.},
    author    = {David Edgar Araiz Huarte},
    copyright = {https://doi.org/10.56398/ajacieda.00071},
    language  = {esp},
    title     = {La Inteligencia Artificial como agente contaminante: concepto jurídico, impacto ambiental y futura regulación},
    year      = {2023}
}

// ============================== Reverse Engineering ==============================

@online{BinarySearchGitHub,
    author  = {James McDermott, Krishna Vedala},
    title   = {TheAlgorithms in C GitHub Binary Search},
    url     = {https://github.com/TheAlgorithms/C/blob/master/searching/binary_search.c},
    urldate = {2023-10-16},
    note    = {Last visited on October 16, 2023},
    year    = {2020}
}

@article{BrownTomB2020LMaF,
    abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and
                 benchmarks by pre-training on a large corpus of text followed by fine-tuning on
                 a specific task. While typically task-agnostic in architecture, this method
                 still requires task-specific fine-tuning datasets of thousands or tens of
                 thousands of examples. By contrast, humans can generally perform a new language
                 task from only a few examples or from simple instructions - something which
                 current NLP systems still largely struggle to do. Here we show that scaling up
                 language models greatly improves task-agnostic, few-shot performance, sometimes
                 even reaching competitiveness with prior state-of-the-art fine-tuning
                 approaches. Specifically, we train GPT-3, an autoregressive language model with
                 175 billion parameters, 10x more than any previous non-sparse language model,
                 and test its performance in the few-shot setting. For all tasks, GPT-3 is
                 applied without any gradient updates or fine-tuning, with tasks and few-shot
                 demonstrations specified purely via text interaction with the model. GPT-3
                 achieves strong performance on many NLP datasets, including translation,
                 question-answering, and cloze tasks, as well as several tasks that require
                 on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
                 novel word in a sentence, or performing 3-digit arithmetic. At the same time,
                 we also identify some datasets where GPT-3's few-shot learning still struggles,
                 as well as some datasets where GPT-3 faces methodological issues related to
                 training on large web corpora. Finally, we find that GPT-3 can generate samples
                 of news articles which human evaluators have difficulty distinguishing from
                 articles written by humans. We discuss broader societal impacts of this finding
                 and of GPT-3 in general.},
    author    = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
    keywords  = {Computer Science - Computation and Language},
    language  = {eng},
    title     = {Language Models are Few-Shot Learners},
    year      = {2020}
}

@online{CoWorking,
    author   = {Wikipedia},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title    = {Trabajo cooperativo},
    url      = {https://es.wikipedia.org/wiki/Trabajo_cooperativo},
    urldate  = {2023-10-09},
    note     = {Last visited on October 9, 2023},
    year     = {2023}
}

@article{DettmersTim2023QEFo,
    abstract  = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
    author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
    address   = {Ithaca},
    journal   = {arXiv.org},
    copyright = {2023. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    keywords  = {Benchmarks ; Chatbots ; Computer Science - Learning ; Datasets ; Large language models ; Mathematical models ; Parameters ; Performance evaluation},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {QLoRA: Efficient Finetuning of Quantized LLMs},
    year      = {2023}
}

@online{ECTS,
    author  = {{Ministerio de Educación y Formación Profesional}},
    title   = {El sistema universitario español},
    url     = {https://www.educacionyfp.gob.es/italia/dam/jcr:b53864d2-65a3-4526-abf4-61ef02f5be34/el-sistema-universitario-espa-ol2.pdf},
    urldate = {2023-10-02},
    note    = {Last visited on October 2, 2023},
    year    = {2023}
}

@online{EspañaDigital2025,
    author  = {{Ministerio de Transformación Digital}},
    title   = {España Digital 2025},
    url     = {https://avancedigital.mineco.gob.es/programas-avance-digital/paginas/espana-digital-2025.aspx},
    urldate = {2023-12-27},
    note    = {Las visited on December 27, 2023},
    year    = {2023}
}

@online{EvolutionLLM,
    author  = {{Eric Topol}},
    title   = {When M.D. is a Machine Doctor},
    url     = {https://erictopol.substack.com/p/when-md-is-a-machine-doctor},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2023}
}

@online{FasesIngineriaInversa,
    author  = {Edwin H. Garzón},
    title   = {Descifrando el arte de la Ingeniería Inversa en aplicaciones: una mirada profunda},
    url     = {https://blog.isecauditors.com/2023/11/descifrando-el-arte-de-la-ingenieria-inversa-en-aplicaciones.html},
    urldate = {2023-01-02},
    note    = {Last visited on January 2, 2023},
    year    = {2023}
}

@online{Finetuning,
    author  = {{Sebastian Raschka}},
    title   = {Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters},
    url     = {https://lightning.ai/pages/community/article/understanding-llama-adapters/},
    urldate = {2024-01-02},
    note    = {Last visited on January 2, 2024},
    year    = {2023}
}

@online{Git,
    author  = {{Atlassian}},
    title   = {Qué es Git},
    url     = {https://www.atlassian.com/es/git/tutorials/what-is-git},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{Github,
    author  = {{Wikipedia}},
    title   = {GitHub},
    url     = {https://en.wikipedia.org/wiki/GitHub},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{GithubCopilot,
    author  = {{Yúbal Fernández}},
    title   = {Qué es Copilot de GitHub y cómo funciona esta inteligencia artificial que te ayuda a programar},
    url     = {https://www.xataka.com/basics/que-copilot-github-como-funciona-esta-inteligencia-artificial-que-te-ayuda-a-programar},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{Glassdoor,
    author   = {Glassdoor},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title    = {Security | Glassdoor},
    url      = {https://www.glassdoor.es/Sueldos/index.htm},
    urldate  = {2023-10-08},
    note     = {Last visited on October 8, 2023},
    year     = {2023}
}

@online{HolaMundoCode,
    author  = {mauriciobrito7},
    title   = {Aprender LenguajeC},
    url     = {https://github.com/mauriciobrito7/Lenguaje-C/blob/master/1_HolaMundo/HolaMundo_HelloWorld.c},
    urldate = {2023-01-02},
    note    = {Last visited on January 2, 2023},
    year    = {2016}
}

@article{HuEdwardJ2021LLAo,
    abstract  = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
    author    = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
    address   = {Ithaca},
    copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Adaptation ; Domains ; Large language models ; Mathematical models ; Natural language processing ; Parameters ; Training},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {LoRA: Low-Rank Adaptation of Large Language Models},
    year      = {2021}
}

@online{IDAPro_Wikipedia,
    author  = {Wikipedia},
    title   = {Interactive Disassembler},
    url     = {https://es.wikipedia.org/wiki/Interactive_Disassembler},
    urldate = {2023-10-16},
    note    = {Last visited on October 16, 2023},
    year    = {2019}
}

@online{IndustriaSoftware,
    author  = {{Laura del Rio}},
    title   = {La industria del software crecerá más del doble del PIB mundial},
    url     = {https://www.computing.es/informes/la-industria-del-software-crecera-mas-del-doble-del-pib-mundial/},
    urldate = {2023-12-27},
    note    = {Last visited on December 27, 2023},
    year    = {2023}
}

@online{IngenieriaInversa,
    author  = {{Structuralia}},
    title   = {¿Qué es la ingeniería inversa? ¿Es ética?},
    url     = {https://blog.structuralia.com/que-es-la-ingenier%C3%ADa-inversa},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2022}
}

@online{LeyPropiedadIntelectual,
    author  = {{BOE}},
    title   = {Ley de Propiedad Intelectual},
    url     = {https://www.boe.es/buscar/act.php?id=BOE-A-1996-8930#a100},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {1996}
}

@online{LightningAI,
    author  = {{Lightning AI}},
    title   = {Lightning AI},
    url     = {https://lightning.ai/pages/about/},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{litGPT,
    author  = {{Lightning AI}},
    title   = {lit-gpt},
    url     = {https://github.com/Lightning-AI/lit-gpt},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{MetodoAgile,
    author  = {Sandra Garrido Sotomayor},
    title   = {Las metodologías ágiles más utilizadas y sus ventajas dentro de la empresa},
    url     = {https://www.iebschool.com/blog/que-son-metodologias-agiles-agile-scrum/},
    urldate = {2023-11-26},
    note    = {Last visited on November 26, 2023},
    year    = {2023}
}

@article{NelsonMichaelL2005ASoR,
    title    = {A Survey of Reverse Engineering and Program Comprehension},
    year     = {2005},
    keywords = {Computer Science - Software Engineering},
    abstract = {Reverse engineering has been a standard practice in the hardware community
                for some time. It has only been within the last ten years that reverse
                engineering, or "program comprehension", has grown into the current
                sub-discipline of software engineering. Traditional software engineering is
                primarily focused on the development and design of new software. However, most
                programmers work on software that other people have designed and developed. Up
                to 50% of a software maintainers time can be spent determining the intent of
                source code. The growing demand to reevaluate and reimplement legacy software
                systems, brought on by the proliferation of clientserver and World Wide Web
                technologies, has underscored the need for reverse engineering tools and
                techniques. This paper introduces the terminology of reverse engineering and
                gives some of the obstacles that make reverse engineering difficult. Although
                reverse engineering remains heavily dependent on the human component, a number
                of automated tools are presented that aid the reverse engineer.},
    author   = {Nelson, Michael L},
    language = {eng}
}

@online{ProgramaEuropaDigital,
    author  = {{Unión Europea}},
    title   = {Programa Europa Digital},
    url     = {https://digital-strategy.ec.europa.eu/es/activities/digital-programme},
    urldate = {2023-12-27},
    note    = {Last visited on December 27, 2023},
    year    = {2023}
}

@online{PyTorch,
    author  = {{Wikipedia}},
    title   = {PyTorch},
    url     = {https://es.wikipedia.org/wiki/PyTorch},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{Software,
    author  = {{Arimetrics}},
    title   = {Qué es Software},
    url     = {https://www.arimetrics.com/glosario-digital/software},
    urldate = {2023-12-27},
    note    = {Last visited on December 27, 2023},
    year    = {2023}
}



@online{StableCode,
    author  = {{Stability AI}},
    title   = {Announcing StableCode},
    url     = {https://stability.ai/news/stablecode-llm-generative-ai-coding},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{SupervisedFineTuning,
    author  = {{Jose J. Martinez}},
    title   = {Supervised Fine-tuning: customizing LLMs},
    url     = {https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2023}
}

@online{TecnicasIlegibleBinario,
    author  = {{Irene Díez}},
    title   = {Crea un binario ilegible con estas técnicas - S3lab},
    url     = {https://s3lab.deusto.es/binario-ilegible-tecnicas/},
    urldate = {2023-10-12},
    note    = {Last visited on October 12, 2023},
    year    = {2017}
}

@online{TensorFlow,
    author  = {{Wikipedia}},
    title   = {TensorFlow},
    url     = {https://es.wikipedia.org/wiki/TensorFlow},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{VisualStudioCode,
    author  = {{Wikipedia}},
    title   = {Visual Studio Code},
    url     = {https://en.wikipedia.org/wiki/Visual_Studio_Code},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@article{ZhaoWayneXin2023ASoL,
    abstract  = {Language is essentially a complex, intricate system of human expressions
                 governed by grammatical rules. It poses a significant challenge to develop
                 capable AI algorithms for comprehending and grasping a language. As a major
                 approach, language modeling has been widely studied for language understanding
                 and generation in the past two decades, evolving from statistical language
                 models to neural language models. Recently, pre-trained language models (PLMs)
                 have been proposed by pre-training Transformer models over large-scale corpora,
                 showing strong capabilities in solving various NLP tasks. Since researchers
                 have found that model scaling can lead to performance improvement, they further
                 study the scaling effect by increasing the model size to an even larger size.
                 Interestingly, when the parameter scale exceeds a certain level, these enlarged
                 language models not only achieve a significant performance improvement but also
                 show some special abilities that are not present in small-scale language
                 models. To discriminate the difference in parameter scale, the research
                 community has coined the term large language models (LLM) for the PLMs of
                 significant size. Recently, the research on LLMs has been largely advanced by
                 both academia and industry, and a remarkable progress is the launch of ChatGPT,
                 which has attracted widespread attention from society. The technical evolution
                 of LLMs has been making an important impact on the entire AI community, which
                 would revolutionize the way how we develop and use AI algorithms. In this
                 survey, we review the recent advances of LLMs by introducing the background,
                 key findings, and mainstream techniques. In particular, we focus on four major
                 aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
                 capacity evaluation. Besides, we also summarize the available resources for
                 developing LLMs and discuss the remaining issues for future directions.},
    author    = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
    copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
    language  = {eng},
    title     = {A Survey of Large Language Models},
    year      = {2023}
}

