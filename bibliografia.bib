@article{8330222,
    author    = {Katz, Deborah S. and Ruchti, Jason and Schulte, Eric},
    booktitle = {2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
    title     = {Using recurrent neural networks for decompilation},
    year      = {2018},
    pages     = {346-356},
    abstract  = {Decompilation, recovering source code from binary, is useful in many situations where it is necessary to analyze or understand software for which source code is not available. Source code is much easier for humans to read than binary code, and there are many tools available to analyze source code. Existing decompilation techniques often generate source code that is difficult for humans to understand because the generated code often does not use the coding idioms that programmers use. Differences from human-written code also reduce the effectiveness of analysis tools on the decompiled source code. To address the problem of differences between decompiled code and human-written code, we present a novel technique for decompiling binary code snippets using a model based on Recurrent Neural Networks. The model learns properties and patterns that occur in source code and uses them to produce decompilation output. We train and evaluate our technique on snippets of binary machine code compiled from C source code. The general approach we outline in this paper is not language-specific and requires little or no domain knowledge of a language and its properties or how a compiler operates, making the approach easily extensible to new languages and constructs. Furthermore, the technique can be extended and applied in situations to which traditional decompilers are not targeted, such as for decompilation of isolated binary snippets; fast, on-demand decompilation; domain-specific learned decompilation; optimizing for readability of decompilation; and recovering control flow constructs, comments, and variable or function names. We show that the translations produced by this technique are often accurate or close and can provide a useful picture of the snippet's behavior.},
    doi       = {10.1109/SANER.2018.8330222},
    month     = {March}
}

@book{alma991003132729706711,
    title     = {Reversing : secrets of reverse engineering },
    year      = {2005},
    keywords  = {Enginyeria del software},
    author    = {Eilam, Eldad and Chikofsky, Elliot J.},
    address   = {Indianapolis},
    booktitle = {Reversing : secrets of reverse engineering},
    isbn      = {0764574817},
    language  = {eng},
    publisher = {Wiley}
}

@book{alma991004951313206711,
    title     = {Reverse engineering },
    year      = {1996},
    keywords  = {Software engineering},
    abstract  = {Reverse Engineering brings together in one place important contributions and up-to-date research results in this important area. Reverse Engineering serves as an excellent reference, providing insight into some of the most important issues in the field.},
    author    = {Newcomb, Philip and Wills, Linda},
    address   = {Boston},
    booktitle = {Reverse engineering},
    edition   = {1st ed. 1996.},
    isbn      = {0-585-27477-0},
    language  = {eng},
    publisher = {Kluwer Academic Publishers}
}

@book{alma991004951382906711,
    series    = {Monographs in Computer Science},
    title     = {Reverse Engineering of Object Oriented Code},
    year      = {2005},
    keywords  = {Software engineering},
    abstract  = {In software evolution and maintenance, the ultimate, most reliable description of a system is its source code. Reverse engineering aims at extracting abstract, goal-oriented views from the code, to summarize relevant properties of program computations. Reverse Engineering of Object Oriented Code presents a unifying framework for the analysis of object oriented code. Using Unified Modeling Language (UML) to represent the extracted design diagrams, the book explains how to recover them from object oriented code, thereby enabling developers to better comprehend their product and evaluate the impact of changes to it. Furthermore, it describes the algorithms involved in recovering views and demonstrates some of the techniques that can be employed for their visualization. The presentation is fully self-contained. Topics and Features: *Provides unique, in-depth exposition of the core concepts, principles, and methods behind reverse engineering object oriented code *Explains the techniques and algorithms through numerous examples of object oriented code, the leading programming paradigm *Focuses on fully automated design recovery, and deals with static and dynamic source-code analysis algorithms *Explores code-centered analysis to obtain design diagrams aligned with the implementation *Describes structural and behavioral views to offer a multi-perspective assessment of the system being analyzed *Reports the analysis results in UML, the standard language for representing design diagrams in object oriented program development This new state-of-the-art volume covers core methodologies for reverse engineering object oriented code, allowing for improved control in future code maintenance and modification. It is a significant resource for researchers and software engineers in the areas of reverse engineering, code analysis, object oriented programming, and UML. In addition, it will be invaluable as the reference book for advanced courses in these areas.},
    author    = {Tonella, Paolo. and Potrich, Alessandra.},
    address   = {New York, NY},
    edition   = {1st ed. 2005.},
    isbn      = {1-280-19032-9},
    language  = {eng},
    publisher = {Springer New York}
}

@article{Armengol-EstapéJordi2021LCtx,
    abstract  = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
    author    = {Armengol-Estapé, Jordi and O'Boyle, Michael F P},
    address   = {Ithaca},
    copyright = {2022. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Deep learning},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {Learning C to x86 Translation: An Experiment in Neural Compilation},
    year      = {2021}
}

@article{Artículo_doctrinal,
    abstract  = {La inteligencia artificial está suscitando intensos debates en
                 el conjunto de la doctrina sobre su compatibilidad con la ética humanista
                 imperante en la sociedad occidental. Esta perspectiva antropocéntrica del
                 cambio de paradigma planteado por la inteligencia artificial no agota sin
                 embargo la hilera de consecuencias perniciosas que la misma puede acompañar.
                 Así pues, el medio ambiente también se erige como un bien jurídico a proteger
                 frente al fenómeno de esta nueva tecnología. La ingente cantidad de energía demandada
                 por algunos de los modelos de inteligencia artificial erosiona de raíz cualquier objetivo
                 de eficiencia energética que pueda llegar a plantearse en el campo de las TIC. Por ello,
                 el objetivo de este trabajo será el consistente en analizar este nuevo reto al que deberá
                 enfrentarse, más pronto que tarde, el Derecho ambiental.},
    author    = {David Edgar Araiz Huarte},
    copyright = {https://doi.org/10.56398/ajacieda.00071},
    language  = {esp},
    title     = {La Inteligencia Artificial como agente contaminante: concepto jurídico, impacto ambiental y futura regulación},
    year      = {2023}
}

@online{BinarySearchGitHub,
    author  = {James McDermott, Krishna Vedala},
    title   = {TheAlgorithms in C GitHub Binary Search},
    url     = {https://github.com/TheAlgorithms/C/blob/master/searching/binary_search.c},
    urldate = {2023-10-16},
    note    = {Last visited on October 16, 2023},
    year    = {2020}
}

@article{BrownTomB2020LMaF,
    abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and
                 benchmarks by pre-training on a large corpus of text followed by fine-tuning on
                 a specific task. While typically task-agnostic in architecture, this method
                 still requires task-specific fine-tuning datasets of thousands or tens of
                 thousands of examples. By contrast, humans can generally perform a new language
                 task from only a few examples or from simple instructions - something which
                 current NLP systems still largely struggle to do. Here we show that scaling up
                 language models greatly improves task-agnostic, few-shot performance, sometimes
                 even reaching competitiveness with prior state-of-the-art fine-tuning
                 approaches. Specifically, we train GPT-3, an autoregressive language model with
                 175 billion parameters, 10x more than any previous non-sparse language model,
                 and test its performance in the few-shot setting. For all tasks, GPT-3 is
                 applied without any gradient updates or fine-tuning, with tasks and few-shot
                 demonstrations specified purely via text interaction with the model. GPT-3
                 achieves strong performance on many NLP datasets, including translation,
                 question-answering, and cloze tasks, as well as several tasks that require
                 on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
                 novel word in a sentence, or performing 3-digit arithmetic. At the same time,
                 we also identify some datasets where GPT-3's few-shot learning still struggles,
                 as well as some datasets where GPT-3 faces methodological issues related to
                 training on large web corpora. Finally, we find that GPT-3 can generate samples
                 of news articles which human evaluators have difficulty distinguishing from
                 articles written by humans. We discuss broader societal impacts of this finding
                 and of GPT-3 in general.},
    author    = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
    keywords  = {Computer Science - Computation and Language},
    language  = {eng},
    title     = {Language Models are Few-Shot Learners},
    year      = {2020}
}

@online{CoWorking,
    author   = {Wikipedia},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title    = {Trabajo cooperativo},
    url      = {https://es.wikipedia.org/wiki/Trabajo_cooperativo},
    urldate  = {2023-10-09},
    note     = {Last visited on October 9, 2023},
    year     = {2023}
}

@article{DettmersTim2023QEFo,
    abstract  = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
    author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
    address   = {Ithaca},
    journal   = {arXiv.org},
    copyright = {2023. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    keywords  = {Benchmarks ; Chatbots ; Computer Science - Learning ; Datasets ; Large language models ; Mathematical models ; Parameters ; Performance evaluation},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {QLoRA: Efficient Finetuning of Quantized LLMs},
    year      = {2023}
}

@online{ECTS,
    author  = {{Ministerio de Educación y Formación Profesional}},
    title   = {El sistema universitario español},
    url     = {https://www.educacionyfp.gob.es/italia/dam/jcr:b53864d2-65a3-4526-abf4-61ef02f5be34/el-sistema-universitario-espa-ol2.pdf},
    urldate = {2023-10-02},
    note    = {Last visited on October 2, 2023},
    year    = {2023}
}

@online{EuclidianCode,
    author  = {{Wikibooks}},
    title   = {x86 Disassembly/Optimization Examples},
    url     = {https://en.wikibooks.org/wiki/X86_Disassembly/Optimization_Examples#Example:_Optimized_vs_Non-Optimized_Code},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{EvolutionLLM,
    author  = {{Eric Topol}},
    title   = {When M.D. is a Machine Doctor},
    url     = {https://erictopol.substack.com/p/when-md-is-a-machine-doctor},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2023}
}

@online{FasesIngineriaInversa,
    author  = {Edwin H. Garzón},
    title   = {Descifrando el arte de la Ingeniería Inversa en aplicaciones: una mirada profunda},
    url     = {https://blog.isecauditors.com/2023/11/descifrando-el-arte-de-la-ingenieria-inversa-en-aplicaciones.html},
    urldate = {2023-01-02},
    note    = {Last visited on January 2, 2023},
    year    = {2023}
}

@online{Finetuning,
    author  = {{Sebastian Raschka}},
    title   = {Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters},
    url     = {https://lightning.ai/pages/community/article/understanding-llama-adapters/},
    urldate = {2024-01-02},
    note    = {Last visited on January 2, 2024},
    year    = {2023}
}

@article{fu2021nbref,
    title  = {N-Bref : A High-fidelity Decompiler Exploiting Programming Structures },
    author = {Cheng Fu and Kunlin Yang and Xinyun Chen and Yuandong Tian and Jishen Zhao},
    year   = {2021},
    url    = {https://openreview.net/forum?id=6GkL6qM3LV}
}

@article{FuCheng2019ANPD,
    abstract  = {Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\% program accuracy.},
    author    = {Fu, Cheng and Chen, Huili and Liu, Haolan and Chen, Xinyun and Tian, Yuandong and Koushanfar, Farinaz and Zhao, Jishen},
    address   = {Ithaca},
    copyright = {2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Accuracy ; Computer Science - Learning ; Computer Science - Programming Languages ; Computer security ; Error correction ; Iterative methods ; Malware ; Model accuracy ; Reverse engineering ; Semantics ; Software reliability},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {A Neural-based Program Decompiler},
    year      = {2019}
}

@online{Git,
    author  = {{Atlassian}},
    title   = {Qué es Git},
    url     = {https://www.atlassian.com/es/git/tutorials/what-is-git},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{Github,
    author  = {{Wikipedia}},
    title   = {GitHub},
    url     = {https://en.wikipedia.org/wiki/GitHub},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{GithubCopilot,
    author  = {{Yúbal Fernández}},
    title   = {Qué es Copilot de GitHub y cómo funciona esta inteligencia artificial que te ayuda a programar},
    url     = {https://www.xataka.com/basics/que-copilot-github-como-funciona-esta-inteligencia-artificial-que-te-ayuda-a-programar},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@online{Glassdoor,
    author   = {Glassdoor},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title    = {Security | Glassdoor},
    url      = {https://www.glassdoor.es/Sueldos/index.htm},
    urldate  = {2023-10-08},
    note     = {Last visited on October 8, 2023},
    year     = {2023}
}

@online{HolaMundoCode,
    author  = {mauriciobrito7},
    title   = {Aprender LenguajeC},
    url     = {https://github.com/mauriciobrito7/Lenguaje-C/blob/master/1_HolaMundo/HolaMundo_HelloWorld.c},
    urldate = {2023-01-02},
    note    = {Last visited on January 2, 2023},
    year    = {2016}
}

@article{HosseiniIman2022BtCR,
    abstract  = {The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation.},
    author    = {Hosseini, Iman and Dolan-Gavitt, Brendan},
    address   = {Ithaca},
    copyright = {2022. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Assembly ; Computer Science - Computation and Language ; Computer Science - Cryptography and Security ; Computer Science - Programming Languages ; Domains ; Languages ; Machine translation ; Reverse engineering ; Software engineering ; Training},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {Beyond the C: Retargetable Decompilation using Neural Machine Translation},
    year      = {2022}
}

@article{HuEdwardJ2021LLAo,
    abstract  = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
    author    = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
    address   = {Ithaca},
    copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Adaptation ; Domains ; Large language models ; Mathematical models ; Natural language processing ; Parameters ; Training},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {LoRA: Low-Rank Adaptation of Large Language Models},
    year      = {2021}
}

@online{IDAPro_Wikipedia,
    author  = {Wikipedia},
    title   = {Interactive Disassembler},
    url     = {https://es.wikipedia.org/wiki/Interactive_Disassembler},
    urldate = {2023-10-16},
    note    = {Last visited on October 16, 2023},
    year    = {2019}
}

@online{IndustriaSoftware,
    author  = {{Laura del Rio}},
    title   = {La industria del software crecerá más del doble del PIB mundial},
    url     = {https://www.computing.es/informes/la-industria-del-software-crecera-mas-del-doble-del-pib-mundial/},
    urldate = {2023-12-27},
    note    = {Last visited on December 27, 2023},
    year    = {2023}
}

@online{IngenieriaInversa,
    author  = {{Structuralia}},
    title   = {¿Qué es la ingeniería inversa? ¿Es ética?},
    url     = {https://blog.structuralia.com/que-es-la-ingenier%C3%ADa-inversa},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2022}
}

@article{KatzOmer2019TND,
    abstract  = {We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively.},
    author    = {Katz, Omer and Olshaker, Yuval and Goldberg, Yoav and Yahav, Eran},
    address   = {Ithaca},
    copyright = {2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
    issn      = {2331-8422},
    journal   = {arXiv.org},
    keywords  = {Computer Science - Learning ; Computer Science - Programming Languages ; Machine translation ; Malware ; Programming languages ; Source code},
    language  = {eng},
    publisher = {Cornell University Library, arXiv.org},
    title     = {Towards Neural Decompilation},
    year      = {2019}
}

@online{LeyPropiedadIntelectual,
    author  = {{BOE}},
    title   = {Ley de Propiedad Intelectual},
    url     = {https://www.boe.es/buscar/act.php?id=BOE-A-1996-8930#a100},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {1996}
}

@article{LiangRuigang2021Naan,
    abstract  = {Decompilation aims to analyze and transform low-level program language (PL) codes such as binary code or assembly code to obtain an equivalent high-level PL. Decompilation plays a vital role in the cyberspace security fields such as software vulnerability discovery and analysis, malicious code detection and analysis, and software engineering fields such as source code analysis, optimization, and cross-language cross-operating system migration. Unfortunately, the existing decompilers mainly rely on experts to write rules, which leads to bottlenecks such as low scalability, development difficulties, and long cycles. The generated high-level PL codes often violate the code writing specifications. Further, their readability is still relatively low. The problems mentioned above hinder the efficiency of advanced applications (e.g., vulnerability discovery) based on decompiled high-level PL codes.In this paper, we propose a decompilation approach based on the attention-based neural machine translation (NMT) mechanism, which converts low-level PL into high-level PL while acquiring legibility and keeping functionally similar. To compensate for the information asymmetry between the low-level and high-level PL, a translation method based on basic operations of low-level PL is designed. This method improves the generalization of the NMT model and captures the translation rules between PLs more accurately and efficiently. Besides, we implement a neural decompilation framework called Neutron. The evaluation of two practical applications shows that Neutron’s average program accuracy is 96.96%, which is better than the traditional NMT model.},
    author    = {Liang, Ruigang and Cao, Ying and Hu, Peiwei and Chen, Kai},
    address   = {Singapore},
    copyright = {The Author(s) 2021},
    issn      = {2096-4862},
    journal   = {Cybersecurity (Singapore)},
    keywords  = {Assembly language ; Attention ; Binary code ; Binary codes ; Code (cryptography) ; Computer engineering. Computer hardware ; Computer Science ; Decompilation ; Decompiler ; electrical engineering, electronic engineering, information engineering ; Electronic computers. Computer science ; engineering and technology ; information systems ; Internet ; Legibility ; LSTM ; Machine translation ; Malware ; Optimization ; Programming language ; QA75.5-76.95 ; Scalability ; Software ; Software engineering ; Software reliability ; Source code ; System migration ; TK7885-7895 ; Translation},
    language  = {eng},
    number    = {1},
    pages     = {1-13},
    publisher = {Springer Singapore},
    title     = {Neutron: an attention-based neural decompiler},
    volume    = {4},
    year      = {2021}
}

@online{LightningAI,
    author  = {{Lightning AI}},
    title   = {Lightning AI},
    url     = {https://lightning.ai/pages/about/},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{lightningiaPricing,
    author  = {{Lightning AI}},
    title   = {Pricing},
    url     = {https://lightning.ai/pricing},
    urldate = {2024-01-08},
    note    = {Last visited on January 8, 2024},
    year    = {2023}
}

@online{litGPT,
    author  = {{Lightning AI}},
    title   = {lit-gpt},
    url     = {https://github.com/Lightning-AI/lit-gpt},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{MetodoAgile,
    author  = {Sandra Garrido Sotomayor},
    title   = {Las metodologías ágiles más utilizadas y sus ventajas dentro de la empresa},
    url     = {https://www.iebschool.com/blog/que-son-metodologias-agiles-agile-scrum/},
    urldate = {2023-11-26},
    note    = {Last visited on November 26, 2023},
    year    = {2023}
}

@article{NelsonMichaelL2005ASoR,
    title    = {A Survey of Reverse Engineering and Program Comprehension},
    year     = {2005},
    keywords = {Computer Science - Software Engineering},
    abstract = {Reverse engineering has been a standard practice in the hardware community
                for some time. It has only been within the last ten years that reverse
                engineering, or "program comprehension", has grown into the current
                sub-discipline of software engineering. Traditional software engineering is
                primarily focused on the development and design of new software. However, most
                programmers work on software that other people have designed and developed. Up
                to 50% of a software maintainers time can be spent determining the intent of
                source code. The growing demand to reevaluate and reimplement legacy software
                systems, brought on by the proliferation of clientserver and World Wide Web
                technologies, has underscored the need for reverse engineering tools and
                techniques. This paper introduces the terminology of reverse engineering and
                gives some of the obstacles that make reverse engineering difficult. Although
                reverse engineering remains heavily dependent on the human component, a number
                of automated tools are presented that aid the reverse engineer.},
    author   = {Nelson, Michael L},
    language = {eng}
}


@online{PyTorch,
    author  = {{Wikipedia}},
    title   = {PyTorch},
    url     = {https://es.wikipedia.org/wiki/PyTorch},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{StableCode,
    author  = {{Stability AI}},
    title   = {Announcing StableCode},
    url     = {https://stability.ai/news/stablecode-llm-generative-ai-coding},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{SupervisedFineTuning,
    author  = {{Jose J. Martinez}},
    title   = {Supervised Fine-tuning: customizing LLMs},
    url     = {https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3},
    urldate = {2024-01-01},
    note    = {Last visited on January 1, 2024},
    year    = {2023}
}

@online{TecnicasIlegibleBinario,
    author  = {{Irene Díez}},
    title   = {Crea un binario ilegible con estas técnicas - S3lab},
    url     = {https://s3lab.deusto.es/binario-ilegible-tecnicas/},
    urldate = {2023-10-12},
    note    = {Last visited on October 12, 2023},
    year    = {2017}
}

@online{TensorFlow,
    author  = {{Wikipedia}},
    title   = {TensorFlow},
    url     = {https://es.wikipedia.org/wiki/TensorFlow},
    urldate = {2024-01-07},
    note    = {Last visited on January 7, 2024},
    year    = {2023}
}

@online{VisualStudioCode,
    author  = {{Wikipedia}},
    title   = {Visual Studio Code},
    url     = {https://en.wikipedia.org/wiki/Visual_Studio_Code},
    urldate = {2023-12-30},
    note    = {Last visited on December 30, 2023},
    year    = {2023}
}

@article{ZhaoWayneXin2023ASoL,
    abstract  = {Language is essentially a complex, intricate system of human expressions
                 governed by grammatical rules. It poses a significant challenge to develop
                 capable AI algorithms for comprehending and grasping a language. As a major
                 approach, language modeling has been widely studied for language understanding
                 and generation in the past two decades, evolving from statistical language
                 models to neural language models. Recently, pre-trained language models (PLMs)
                 have been proposed by pre-training Transformer models over large-scale corpora,
                 showing strong capabilities in solving various NLP tasks. Since researchers
                 have found that model scaling can lead to performance improvement, they further
                 study the scaling effect by increasing the model size to an even larger size.
                 Interestingly, when the parameter scale exceeds a certain level, these enlarged
                 language models not only achieve a significant performance improvement but also
                 show some special abilities that are not present in small-scale language
                 models. To discriminate the difference in parameter scale, the research
                 community has coined the term large language models (LLM) for the PLMs of
                 significant size. Recently, the research on LLMs has been largely advanced by
                 both academia and industry, and a remarkable progress is the launch of ChatGPT,
                 which has attracted widespread attention from society. The technical evolution
                 of LLMs has been making an important impact on the entire AI community, which
                 would revolutionize the way how we develop and use AI algorithms. In this
                 survey, we review the recent advances of LLMs by introducing the background,
                 key findings, and mainstream techniques. In particular, we focus on four major
                 aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
                 capacity evaluation. Besides, we also summarize the available resources for
                 developing LLMs and discuss the remaining issues for future directions.},
    author    = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
    copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
    language  = {eng},
    title     = {A Survey of Large Language Models},
    year      = {2023}
}
