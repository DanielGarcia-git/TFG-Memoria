\chapter{Resultados}
\label{cap:resultados}

% Corregido 02/01/2023
% Revisado 14/01/2024

En este capítulo se presentan los resultados obtenidos en el desarrollo de este trabajo. 
En la sección \ref{sec:evaluacion} se describe la metodología de evaluación utilizada 
para medir el desempeño de los modelos. En la sección \ref{sec:resultados_finales} se
presentan los resultados finales obtenidos con el modelo \textit{Stable Code} después 
de realizarle un \textit{fine-tuning}. Finalmente, en la sección \ref{sec:analisis_resultados}
se realiza un análisis de los resultados obtenidos.

\section{Metodología de evaluación}
\label{sec:evaluacion}

% Corregido 08/01/2024
% Revisado 14/01/2024

Para poder evaluar la calidad de los resultados se han definido una serie de criterios que
nos permitirán evaluar los resultados obtenidos. Estos criterios son los siguientes:

\begin{enumerate}
    \item \textbf{Criterio 1:} el modelo debe de ser capaz de generar código en C que sea
        compilable y ejecutable.
    \item \textbf{Criterio 2:} el modelo debe de ser capaz de generar código en C que sea
        funcionalmente correcto.
\end{enumerate}

Por lo tanto, si el resultado cumple con el primer criterio, diremos que es un resultado aceptable,
ya que ha generado un código sintácticamente correcto. Si el resultado cumple con el segundo criterio
diremos que es un resultado excelente, ya que ha generado un código sintácticamente correcto y funcionalmente
correcto. Si no cumple con ninguno de los dos criterios, diremos que es un resultado malo.

\section{Resultados finales}
\label{sec:resultados_finales}

% Corregido 10/01/2024
% Revisado 14/01/2024

Después de haber ejecutado el \textit{fine-tuning} de nuestro modelo, vamos a hacer las mismas pruebas
que hicimos en el capítulo \ref{cap:viabilidad_hipotesis} para comprobar si el modelo es capaz de
generar código en C que sea compilable y ejecutable. Para ello, vamos a utilizar el mismo conjunto
de datos. Diferenciare entre las diferentes estrategias de entrenamiento definidas en el capítulo
\ref{cap:estrategia_entrenamiento}.

Para poder comprobar el resultado lanzaremos el comando que se describe en el código \ref{code:comando_lora},
en la parte de la opcion \mintinline{bash}|--prompt| pondremos el siguiente texto:

\begin{quote}
    \textit{``Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request. \newline
    Instruction: Generates C code from this assembler code \newline
    Input: \textbf{El codigo en ensamblador}\newline
    Response:''}
\end{quote}

Así mismo iremos cambiando los directorios de estrategia dependiendo de que estrategia estemos probando
en cada momento.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
$HOME/soft/bin/python generate/lora.py 
    --prompt "" 
    --checkpoint_dir checkpoints/stabilityai/stablecode-completion-alpha-3b 
    --lora_path out/lora/Estrategia_1/lit_model_lora_finetuned.pth 
    \end{minted}
    \caption[Comando lanzado para generar una salida dado un \textit{prompt} utilizando los pesos refinados por el \textit{fine-tuning}]{Comando lanzado para generar una salida dado un \textit{prompt} utilizando los pesos refinados por el \textit{fine-tuning} (Elaboración propia)}
    \label{code:comando_lora}
\end{mycode}

\subsection{Cluster AC}
\label{subsec:cluster_ac}

\subsubsection{Estrategia 1}
\label{subsubsec:resultados:estrategia_1}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 1

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 1 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.25 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.65 segundos
    \item \textbf{Tiempo de inferencia:} 3.31 segundos, 30.24 tokens por segundo
    \item \textbf{Memoria utilizada:} 2.20 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
The function endbr64 returns to the caller by following the function chain with the function endbr64, a function that returns an error code. The function endbr64 calls the function puts. The function puts calls puts with the argument line, which is a pointer to the line. The function puts calls the function puts, a function that calls  put to print the line. The function puts calls the function puts, a function that calls put to print the line. The
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 1 y utilizando como entrada el programa de \textit{Hello World}]{Salida del modelo entrenado con la estrategia 1 y utilizando como entrada el programa de \textit{Hello World} (Elaboración propia)}
    \label{code:Prueba1_Estrategia1_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 1 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.25 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.66 segundos
    \item \textbf{Tiempo de inferencia:} 4.11 segundos, 24.33 tokens por segundo
    \item \textbf{Memoria utilizada:} 3.78 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
The function endbr64 returns the absolute value of the least significant bit of the value in EBX on success, and 1 on failure. On failure, endbr64 returns the value to the sign of the operand in EBX.

### Instruction:
The function mov ecx, EBX represents the value to the least significant bit of 8 byte at address EBX. The value in EBX is 1 on success, and 0 if the least significant
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 1 y utilizando como entrada el programa de \textit{Binary Search}]{Salida del modelo entrenado con la estrategia 1 y utilizando como entrada el programa de \textit{Binary Search} (Elaboración propia)}
    \label{code:Prueba2_Estrategia1_ClusterAC}
\end{mycode}

\subsubsection{Estrategia 2}
\label{subsubsec:resultados:estrategia_2}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 2

Como he mencionado en el capítulo \ref{cap:estrategia_entrenamiento}, la estrategia 2
es una estrategia que se ha diseñado para intentar mejorar los resultados de la estrategia 1
y que se compone de dos conjuntos de entrenamiento, una más pequeño con 100 000 muestras y otro
que contiene alrededor de 500 000 muestras. En esta sección vamos a ver los resultados obtenidos
con la estrategia 2 usando estos dos conjuntos de entrenamiento y probando con los dos programas
de prueba.

\subsubsection{Conjunto de entrenamiento de 100 000 muestras}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.42 segundos
    \item \textbf{Tiempo de inferencia:} 2.56 segundos, 39.12 tokens por segundo
    \item \textbf{Memoria utilizada:} 5.82 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia2_1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
void ncecl3e(struct sub_data * sub_data) { 
    sub_data = sub_data - 0x0b; 
    union sub_data * sub_data = sub_data - 0x0b; 
    sub_data - & sub_data - sub_data - sub_sub_data = - 0x4f800010b880000000e0000007d
}
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 2 (100 000 muestras) y utilizando como entrada el programa de \textit{Hello World}]{Salida del modelo entrenado con la estrategia 2 (100 000 muestras) y utilizando como entrada el programa de \textit{Hello World} (Elaboración propia)}
    \label{code:Prueba1_Estrategia2_1_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.43 segundos
    \item \textbf{Tiempo de inferencia:} 3.16 segundos, 31.66 tokens por segundo
    \item \textbf{Memoria utilizada:} 7.58 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia2_1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
%% % % rsp % rsp % rsp ***************************************************************** % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp rsp rsp % rsp % rsp rsp % rsp % rsp rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp rsp rsp % rsp % rn % rsp rsp % rsp % rsp % Resp % rsp % rsp % rsp rsp % rsp <%
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 2 (100 000 muestras) y utilizando como entrada el programa de \textit{Binary Search}]{Salida del modelo entrenado con la estrategia 2 (100 000 muestras) y utilizando como entrada el programa de \textit{Binary Search} (Elaboración propia)}
    \label{code:Prueba2_Estrategia2_1_ClusterAC}
\end{mycode}

\subsubsection{Conjunto de entrenamiento de 500 000 muestras}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.15 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.37 segundos
    \item \textbf{Tiempo de inferencia:} 2.52 segundos, 39.65 tokens por segundo
    \item \textbf{Memoria utilizada:} 5.82 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia2_2_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
void ncecl3e(struct sub_data * sub_data) {
    sub_data = sub_data - 0x0b; union sub_data * sub_data = sub_data - 0x0b;
    sub_data - & sub_data - sub_data - sub_sub_data = - 0x4f800010b880000000e0000007d
}
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 2 (500 000 muestras) y utilizando como entrada el programa de \textit{Hello World}]{Salida del modelo entrenado con la estrategia 2 (500 000 muestras) y utilizando como entrada el programa de \textit{Hello World} (Elaboración propia)}
    \label{code:Prueba1_Estrategia2_2_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.36 segundos
    \item \textbf{Tiempo de inferencia:} 3.17 segundos, 31.50 tokens por segundo
    \item \textbf{Memoria utilizada:} 7.58 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia2_2_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
%% % % rsp % rsp % rsp ***************************************************************** % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp rsp rsp % rsp % rsp rsp % rsp % rsp rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp rsp rsp % rsp % rn % rsp rsp % rsp % rsp % Resp % rsp % rsp % rsp rsp % rsp <%
    \end{minted}
    \caption[Salida del modelo entrenado con la estrategia 2 (500 000 muestras) y utilizando como entrada el programa de \textit{Binary Search}]{Salida del modelo entrenado con la estrategia 2 (500 000 muestras) y utilizando como entrada el programa de \textit{Binary Search} (Elaboración propia)}
    \label{code:Prueba2_Estrategia2_2_ClusterAC}
\end{mycode}

\section{Análisis de resultados}
\label{sec:analisis_resultados}

% Corregido 14/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 1 y 2

Una vez visto los resultados obtenidos con las diferentes estrategias de entrenamiento
que se definieron en el capítulo \ref{cap:estrategia_entrenamiento}, vamos a analizar
los resultados obtenidos y ver si cumplen con los criterios de evaluación definidos en
la sección \ref{sec:evaluacion}.

\subsection{Estrategia 1}
\label{subsec:analisis_resultados:estrategia_1}

% Corregido 14/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 1

Por lo que respecta a la estrategia 1 podemos ver que los resultados obtenidos son malos.
Esto es debido a que no obtenemos ninguna salida que sea un código en C, tan solo obtenemos
como salida texto, en este caso texto en inglés que nos intenta explicar el funcionamiento
del ejecutable que le hemos pasado como entrada.

Si entramos en calificar si por lo menos el texto que nos da explica de manera razonada el
funcionamiento del programa, podemos ver que no es así, sí que se aproxima ligeramente al
funcionamiento del programa, pero no es una explicación que nos permita entender con
exactitud la lógica del programa.

\subsection{Estrategia 2}
\label{subsec:analisis_resultados:estrategia_2}

% Corregido 14/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 2

Por lo que respecta a la estrategia 2 podemos ver que los resultados obtenidos son malos.
A diferencia de la estrategia 1, en este caso sí que obtenemos una salida parecida a un
código en C, pero no es un código en C válido, ya que sintácticamente no es correcto.

Así mismo, con el ejemplo del programa de \textit{Hello World} podemos ver que el código
generado intenta ir por las líneas de lo que esperamos obtener como salida, pero por lo que
respecta al programa de \textit{Binary Search} podemos ver que el código generado no tiene
ningún sentido, vemos que solamente repita la palabra de un registro de la CPU.

\subsection{Conclusiones}
\label{subsec:analisis_resultados:conclusiones}

% Corregido 14/01/2024
% TODO:daniel: Revisar las conclusiones

Por lo que respecta a los resultados obtenidos, podemos ver que no cumplen con ninguno de los
criterios de evaluación definidos en la sección \ref{sec:evaluacion}. Por lo tanto, podemos
concluir que la hipótesis planteada en el capítulo \ref{cap:introducion} no es viable.
No obstante, distinguiría en que esta hipótesis no es viable en modelos con pocos parámetros. Como
hemos podido ver en el capítulo \ref{cap:viabilidad_hipotesis} las pruebas hechas en GPT-4 no
eran nada malas y sí que se podía ver que el modelo era capaz de generar código en C que era
compilable y ejecutable.

