\chapter{Resultados}
\label{cap:resultados}

% Corregido 02/01/2023
% TODO:daniel: Revisar el capítulo de resultados

En este capítulo se presentan los resultados obtenidos en el desarrollo de este trabajo. 
En la sección \ref{sec:evaluacion} se describe la metodología de evaluación utilizada 
para medir el desempeño de los modelos. En la sección \ref{sec:resultados_finales} se
presentan los resultados finales obtenidos con el modelo ChatGPT. Finalmente, en la
sección \ref{sec:analisis_resultados} se realiza un análisis de los resultados obtenidos.

\section{Metodología de evaluación}
\label{sec:evaluacion}

% Corregido 08/01/2024
% TODO:daniel: Revisar la metodología de evaluación

Para poder evaluar la calidad de los resultados se han definido una serie de criterios que
nos permitirán evaluar los resultados obtenidos. Estos criterios son los siguientes:

\begin{enumerate}
    \item \textbf{Criterio 1:} el modelo debe de ser capaz de generar código en C que sea
        compilable y ejecutable.
    \item \textbf{Criterio 2:} el modelo debe de ser capaz de generar código en C que sea
        funcionalmente correcto.
\end{enumerate}

Por lo tanto, si el resultado cumple con el primer criterio, diremos que es un resultado aceptable,
ya que ha generado un código sintácticamente correcto. Si el resultado cumple con el segundo criterio
diremos que es un resultado bueno, ya que ha generado un código sintácticamente correcto y funcionalmente
correcto. Si no cumple con ninguno de los dos criterios, diremos que es un resultado malo.

\section{Resultados finales}
\label{sec:resultados_finales}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados finales

Después de haber ejecutado el \textit{fine-tuning} de nuestro modelo, vamos a hacer las mismas pruebas
que hicimos en el capítulo \ref{cap:viabilidad_hipotesis} para comprobar si el modelo es capaz de
generar código en C que sea compilable y ejecutable. Para ello, vamos a utilizar el mismo conjunto
de datos. Diferenciaremos entre los resultados obtenidos en el Clúster del departamento de Arquitectura
de Computadores y los resultados obtenidos en Lightning IA Studio.

Para poder comprobar el resultado lanzaremos el comando que se describe en el código \ref{code:comando_lora},
en la parte de la opcion \mintinline{bash}|--prompt| pondremos el siguiente texto:

\begin{quote}
    \textit{``Below is an instruction that describes a task, paired with an input that provides further context.
    Write a response that appropriately completes the request. \newline
    Instruction: Generates C code from this assembler code \newline
    Input: \textbf{El codigo en ensamblador}\newline
    Response:''}
\end{quote}

Así mismo iremos cambiando los directorios de estrategia dependiendo de que estrategia estemos probando
en cada momento.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
$HOME/soft/bin/python generate/lora.py 
    --prompt "" 
    --checkpoint_dir checkpoints/stabilityai/stablecode-completion-alpha-3b 
    --lora_path out/lora/Estrategia_1/lit_model_lora_finetuned.pth 
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:comando_lora}
\end{mycode}

\subsection{Cluster AC}
\label{subsec:cluster_ac}

\subsubsection{Estrategia 1}
\label{subsubsec:resultados:estrategia_1}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 1

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 1 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.25 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.65 segundos
    \item \textbf{Tiempo de inferencia:} 3.31 segundos, 30.24 tokens por segundo
    \item \textbf{Memoria utilizada:} 2.20 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
The function endbr64 returns to the caller by following the function chain with the function endbr64, a function that returns an error code. The function endbr64 calls the function puts. The function puts calls puts with the argument line, which is a pointer to the line. The function puts calls the function puts, a function that calls  put to print the line. The function puts calls the function puts, a function that calls put to print the line. The
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba1_Estrategia1_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 1 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.25 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.66 segundos
    \item \textbf{Tiempo de inferencia:} 4.11 segundos, 24.33 tokens por segundo
    \item \textbf{Memoria utilizada:} 3.78 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
The function endbr64 returns the absolute value of the least significant bit of the value in EBX on success, and 1 on failure. On failure, endbr64 returns the value to the sign of the operand in EBX.

### Instruction:
The function mov ecx, EBX represents the value to the least significant bit of 8 byte at address EBX. The value in EBX is 1 on success, and 0 if the least significant
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba2_Estrategia1_ClusterAC}
\end{mycode}

\subsubsection{Estrategia 2}
\label{subsubsec:resultados:estrategia_2}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados de la estrategia 2

Como he mencionado en el capítulo \ref{cap:estrategia_entrenamiento}, la estrategia 2
es una estrategia que se ha diseñado para intentar mejorar los resultados de la estrategia 1
y que se compone de dos conjuntos de entrenamiento, una más pequeño con 100 000 muestras y otro
que contiene alrededor de 500 000 muestras. En esta sección vamos a ver los resultados obtenidos
con la estrategia 2 usando estos dos conjuntos de entrenamiento y probando con los dos programas
de prueba.

\subsubsection{Conjunto de entrenamiento de 100 000 muestras}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.42 segundos
    \item \textbf{Tiempo de inferencia:} 2.56 segundos, 39.12 tokens por segundo
    \item \textbf{Memoria utilizada:} 5.82 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia2_1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
void ncecl3e(struct sub_data * sub_data) { 
    sub_data = sub_data - 0x0b; 
    union sub_data * sub_data = sub_data - 0x0b; 
    sub_data - & sub_data - sub_data - sub_sub_data = - 0x4f800010b880000000e0000007d
}
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba1_Estrategia2_1_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.43 segundos
    \item \textbf{Tiempo de inferencia:} 3.16 segundos, 31.66 tokens por segundo
    \item \textbf{Memoria utilizada:} 7.58 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia2_1_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
%% % % rsp % rsp % rsp ***************************************************************** % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp rsp rsp % rsp % rsp rsp % rsp % rsp rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp rsp rsp % rsp % rn % rsp rsp % rsp % rsp % Resp % rsp % rsp % rsp rsp % rsp <%
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba2_Estrategia2_1_ClusterAC}
\end{mycode}

\subsubsection{Conjunto de entrenamiento de 500 000 muestras}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.15 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.37 segundos
    \item \textbf{Tiempo de inferencia:} 2.52 segundos, 39.65 tokens por segundo
    \item \textbf{Memoria utilizada:} 5.82 GB
\end{itemize}

En el código \ref{code:Prueba1_Estrategia2_2_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
void ncecl3e(struct sub_data * sub_data) {
    sub_data = sub_data - 0x0b; union sub_data * sub_data = sub_data - 0x0b;
    sub_data - & sub_data - sub_data - sub_sub_data = - 0x4f800010b880000000e0000007d
}
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba1_Estrategia2_2_ClusterAC}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:} 0.16 segundos
    \item \textbf{Tiempo en cargar los pesos del modelo:} 4.36 segundos
    \item \textbf{Tiempo de inferencia:} 3.17 segundos, 31.50 tokens por segundo
    \item \textbf{Memoria utilizada:} 7.58 GB
\end{itemize}

En el código \ref{code:Prueba2_Estrategia2_2_ClusterAC} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}
%% % % rsp % rsp % rsp ***************************************************************** % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp % rsp rsp rsp % rsp % rsp rsp % rsp % rsp rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp % rsp % rsp rsp % rsp rsp % rsp % rsp rsp rsp % rsp % rn % rsp rsp % rsp % rsp % Resp % rsp % rsp % rsp rsp % rsp <%
    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba2_Estrategia2_2_ClusterAC}
\end{mycode}

\subsection{Lightning IA Studio}
\label{subsec:lightning_ia_studio}

% Corregido 10/01/2024
% TODO:daniel: Revisar los resultados de Lightning IA Studio

Como he mencionado anteriormente, en \textit{Lightning IA Studio} se ha ejecutado el
entrenamiento únicamente con la estrategia 2 y con el conjunto de entrenamiento de
500 000 muestras. En esta sección vamos a ver los resultados obtenidos con esta
configuración.

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Hello World} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:}
    \item \textbf{Tiempo en cargar los pesos del modelo:}
    \item \textbf{Tiempo de inferencia:}
    \item \textbf{Memoria utilizada:}
\end{itemize}

En el código \ref{code:Prueba1_Estrategia2_LightningAI} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}

    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba1_Estrategia2_LightningAI}
\end{mycode}

Ejecutamos los comandos que se describen en el código \ref{code:comando_lora} para
la estrategia 2 y la prueba con el programa de \textit{Binary Search} y obtenemos
las siguientes métricas de generación:

\begin{itemize}
    \item \textbf{Tiempo en instanciar el modelo:}
    \item \textbf{Tiempo en cargar los pesos del modelo:}
    \item \textbf{Tiempo de inferencia:}
    \item \textbf{Memoria utilizada:}
\end{itemize}

En el código \ref{code:Prueba2_Estrategia2_LightningAI} podemos ver el resultado obtenido
en la primera prueba.

\begin{mycode}
    \begin{minted}[fontsize=\scriptsize]{c}

    \end{minted}
    \caption[]{ (Elaboración propia)}
    \label{code:Prueba2_Estrategia2_LightningAI}
\end{mycode}

\section{Análisis de resultados}
\label{sec:analisis_resultados}

Vista general de los resultados obtenidos y análisis de los mismos.