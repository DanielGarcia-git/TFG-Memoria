@article{HuEdwardJ2021LLAo,
abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
address = {Ithaca},
copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Adaptation ; Domains ; Large language models ; Mathematical models ; Natural language processing ; Parameters ; Training},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {LoRA: Low-Rank Adaptation of Large Language Models},
year = {2021},
}

@article{BrownTomB2020LMaF,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.},
author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
keywords = {Computer Science - Computation and Language},
language = {eng},
title = {Language Models are Few-Shot Learners},
year = {2020},
}

@article{ZhaoWayneXin2023ASoL,
abstract = {Language is essentially a complex, intricate system of human expressions
governed by grammatical rules. It poses a significant challenge to develop
capable AI algorithms for comprehending and grasping a language. As a major
approach, language modeling has been widely studied for language understanding
and generation in the past two decades, evolving from statistical language
models to neural language models. Recently, pre-trained language models (PLMs)
have been proposed by pre-training Transformer models over large-scale corpora,
showing strong capabilities in solving various NLP tasks. Since researchers
have found that model scaling can lead to performance improvement, they further
study the scaling effect by increasing the model size to an even larger size.
Interestingly, when the parameter scale exceeds a certain level, these enlarged
language models not only achieve a significant performance improvement but also
show some special abilities that are not present in small-scale language
models. To discriminate the difference in parameter scale, the research
community has coined the term large language models (LLM) for the PLMs of
significant size. Recently, the research on LLMs has been largely advanced by
both academia and industry, and a remarkable progress is the launch of ChatGPT,
which has attracted widespread attention from society. The technical evolution
of LLMs has been making an important impact on the entire AI community, which
would revolutionize the way how we develop and use AI algorithms. In this
survey, we review the recent advances of LLMs by introducing the background,
key findings, and mainstream techniques. In particular, we focus on four major
aspects of LLMs, namely pre-training, adaptation tuning, utilization, and
capacity evaluation. Besides, we also summarize the available resources for
developing LLMs and discuss the remaining issues for future directions.},
author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
title = {A Survey of Large Language Models},
year = {2023},
}

@article{Artículo_doctrinal,
abstract = {La inteligencia artificial está suscitando intensos debates en
el conjunto de la doctrina sobre su compatibilidad con la ética humanista
imperante en la sociedad occidental. Esta perspectiva antropocéntrica del
cambio de paradigma planteado por la inteligencia artificial no agota sin
embargo la hilera de consecuencias perniciosas que la misma puede acompañar.
Así pues, el medio ambiente también se erige como un bien jurídico a proteger
frente al fenómeno de esta nueva tecnología. La ingente cantidad de energía demandada
por algunos de los modelos de inteligencia artificial erosiona de raíz cualquier objetivo
de eficiencia energética que pueda llegar a plantearse en el campo de las TIC. Por ello,
el objetivo de este trabajo será el consistente en analizar este nuevo reto al que deberá
enfrentarse, más pronto que tarde, el Derecho ambiental.},
author = {David Edgar Araiz Huarte},
copyright = {https://doi.org/10.56398/ajacieda.00071},
language = {esp},
title = {La Inteligencia Artificial como agente contaminante: concepto jurídico, impacto ambiental y futura regulación},
year = {2023}
}

// ============================== Reverse Engineering ==============================

@article{NelsonMichaelL2005ASoR,
title = {A Survey of Reverse Engineering and Program Comprehension},
year = {2005},
keywords = {Computer Science - Software Engineering},
abstract = {Reverse engineering has been a standard practice in the hardware community
for some time. It has only been within the last ten years that reverse
engineering, or "program comprehension", has grown into the current
sub-discipline of software engineering. Traditional software engineering is
primarily focused on the development and design of new software. However, most
programmers work on software that other people have designed and developed. Up
to 50% of a software maintainers time can be spent determining the intent of
source code. The growing demand to reevaluate and reimplement legacy software
systems, brought on by the proliferation of clientserver and World Wide Web
technologies, has underscored the need for reverse engineering tools and
techniques. This paper introduces the terminology of reverse engineering and
gives some of the obstacles that make reverse engineering difficult. Although
reverse engineering remains heavily dependent on the human component, a number
of automated tools are presented that aid the reverse engineer.},
author = {Nelson, Michael L},
language = {eng},
}

@book{alma991004951313206711,
title = {Reverse engineering },
year = {1996},
keywords = {Software engineering},
abstract = {Reverse Engineering brings together in one place important contributions and up-to-date research results in this important area. Reverse Engineering serves as an excellent reference, providing insight into some of the most important issues in the field.},
author = {Newcomb, Philip and Wills, Linda},
address = {Boston},
booktitle = {Reverse engineering},
edition = {1st ed. 1996.},
isbn = {0-585-27477-0},
language = {eng},
publisher = {Kluwer Academic Publishers},
}

@book{alma991004951382906711,
series = {Monographs in Computer Science},
title = {Reverse Engineering of Object Oriented Code},
year = {2005},
keywords = {Software engineering},
abstract = {In software evolution and maintenance, the ultimate, most reliable description of a system is its source code. Reverse engineering aims at extracting abstract, goal-oriented views from the code, to summarize relevant properties of program computations. Reverse Engineering of Object Oriented Code presents a unifying framework for the analysis of object oriented code. Using Unified Modeling Language (UML) to represent the extracted design diagrams, the book explains how to recover them from object oriented code, thereby enabling developers to better comprehend their product and evaluate the impact of changes to it. Furthermore, it describes the algorithms involved in recovering views and demonstrates some of the techniques that can be employed for their visualization. The presentation is fully self-contained. Topics and Features: *Provides unique, in-depth exposition of the core concepts, principles, and methods behind reverse engineering object oriented code *Explains the techniques and algorithms through numerous examples of object oriented code, the leading programming paradigm *Focuses on fully automated design recovery, and deals with static and dynamic source-code analysis algorithms *Explores code-centered analysis to obtain design diagrams aligned with the implementation *Describes structural and behavioral views to offer a multi-perspective assessment of the system being analyzed *Reports the analysis results in UML, the standard language for representing design diagrams in object oriented program development This new state-of-the-art volume covers core methodologies for reverse engineering object oriented code, allowing for improved control in future code maintenance and modification. It is a significant resource for researchers and software engineers in the areas of reverse engineering, code analysis, object oriented programming, and UML. In addition, it will be invaluable as the reference book for advanced courses in these areas.},
author = {Tonella, Paolo. and Potrich, Alessandra.},
address = {New York, NY},
edition = {1st ed. 2005.},
isbn = {1-280-19032-9},
language = {eng},
publisher = {Springer New York},
}

@book{alma991003132729706711,
title = {Reversing : secrets of reverse engineering },
year = {2005},
keywords = {Enginyeria del software},
author = {Eilam, Eldad and Chikofsky, Elliot J.},
address = {Indianapolis},
booktitle = {Reversing : secrets of reverse engineering},
isbn = {0764574817},
language = {eng},
publisher = {Wiley},
}


// ============================== Paginas web ==============================
@misc{ECTS,
author = {{Ministerio de Educación y Formación Profesional}},
title = {El sistema universitario español},
url = {https://www.educacionyfp.gob.es/italia/dam/jcr:b53864d2-65a3-4526-abf4-61ef02f5be34/el-sistema-universitario-espa-ol2.pdf},
urldate = {2023-10-02},
year = {2023}
}

@misc{Glassdoor,
    author = {Glassdoor},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title = {Security | Glassdoor},
    url = {https://www.glassdoor.es/Sueldos/index.htm},
    urldate = {2023-10-08},
    year = {2023}
}

@misc{CoWorking,
    author = {Wikipedia},
    abstract = {Glassdoor es un sitio web estadounidense en el que empleados actuales y antiguos hacen reseñas anónimas de empresas.},
    title = {Trabajo cooperativo},
    url = {https://es.wikipedia.org/wiki/Trabajo_cooperativo},
    urldate = {2023-10-09},
    year = {2023}
}

@misc{TecnicasIlegibleBinario,
    author = {{Irene Díez}},
    title = {Crea un binario ilegible con estas técnicas - S3lab},
    url = {https://s3lab.deusto.es/binario-ilegible-tecnicas/},
    urldate = {2023-10-12},
    year = {2017}
}
